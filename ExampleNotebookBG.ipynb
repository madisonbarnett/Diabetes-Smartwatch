{"cells":[{"cell_type":"markdown","id":"f80324f6-c21b-45bb-8e37-5d4b506f1442","metadata":{"id":"f80324f6-c21b-45bb-8e37-5d4b506f1442"},"source":["# For BG smartwatch project"]},{"cell_type":"code","source":[],"metadata":{"id":"bTDuaZCSdu4h"},"id":"bTDuaZCSdu4h","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"nHLPBEsod9Ao"},"id":"nHLPBEsod9Ao"},{"cell_type":"markdown","id":"40bda045-3136-4060-8628-586436e7fd0c","metadata":{"id":"40bda045-3136-4060-8628-586436e7fd0c"},"source":["# Code Setup {example code, some of it needs revising at main loop}[Also, The directory used in the code is just to my specifc machine, please use your own]"]},{"cell_type":"markdown","id":"daef910d-ee54-41ad-a219-c1d916986d90","metadata":{"id":"daef910d-ee54-41ad-a219-c1d916986d90"},"source":["#### 1. Dependency Installation and Imports [Example]"]},{"cell_type":"code","execution_count":null,"id":"a15bcce7-ca0c-4098-9867-f7a24b7e6fab","metadata":{"id":"a15bcce7-ca0c-4098-9867-f7a24b7e6fab"},"outputs":[],"source":["!pip install pandas"]},{"cell_type":"code","execution_count":null,"id":"3b666bb0-8795-4968-90ce-564c5940e15f","metadata":{"id":"3b666bb0-8795-4968-90ce-564c5940e15f"},"outputs":[],"source":["!pip list"]},{"cell_type":"code","execution_count":null,"id":"b57002f9-b5f0-41ab-854a-f608f5302f93","metadata":{"id":"b57002f9-b5f0-41ab-854a-f608f5302f93"},"outputs":[],"source":["# !pip install vitaldb pandas numpy matplotlib scipy scikit-learn\n","\n","import pandas as pd\n","import vitaldb\n","import numpy as np\n","import os\n","from scipy.signal import find_peaks, butter, lfilter\n","from scipy.stats import entropy\n","\n","print(\"All libraries imported successfully.\")\n","\n"]},{"cell_type":"markdown","id":"dd206842-5126-4ac3-82ce-708eaac434d3","metadata":{"id":"dd206842-5126-4ac3-82ce-708eaac434d3"},"source":["#### 2. Configuration [Example]"]},{"cell_type":"markdown","id":"f127c55b-eb59-4a72-ae16-a01f2e446c92","metadata":{"id":"f127c55b-eb59-4a72-ae16-a01f2e446c92"},"source":["This cell sets up all file paths and parameters in a single, easy-to-manage location."]},{"cell_type":"code","execution_count":null,"id":"dc348544-7de8-435b-814f-c2969a063325","metadata":{"id":"dc348544-7de8-435b-814f-c2969a063325"},"outputs":[],"source":["\n","# --- CONFIGURATION ---\n","OUTPUT_DIR = \"./processed_data\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"vitaldb_ppg_extracted_features.csv\")\n","\n","VITALDB_DATA_URL = \"https://api.vitaldb.net/cases\"\n","VITALDB_TRACKS_URL = \"https://api.vitaldb.net/trks\"\n","\n","PPG_SIGNAL_NAME = 'SNUADC/PLETH'\n","SAMPLE_RATE_HZ = 100\n","WINDOW_DURATION_SECONDS = 30\n","SAMPLES_PER_WINDOW = SAMPLE_RATE_HZ * WINDOW_DURATION_SECONDS\n","VALID_WINDOW_MINUTES = 8  # +/- window around 'opstart' for BG stability\n","\n","print(\"Configuration loaded.\")\n","\n"]},{"cell_type":"markdown","id":"8f91125a-c387-4844-9c47-e547f5ced470","metadata":{"id":"8f91125a-c387-4844-9c47-e547f5ced470"},"source":["#### 3. Utility Functions [Example]"]},{"cell_type":"markdown","id":"3a77b31e-fe89-42a1-8263-16827f128373","metadata":{"id":"3a77b31e-fe89-42a1-8263-16827f128373"},"source":["This cell contains the helper functions for bandpass filtering and extracting hand-engineered features from the PPG signal."]},{"cell_type":"code","execution_count":null,"id":"0295dc19-8fc0-4757-8dc1-e47aebd35dba","metadata":{"id":"0295dc19-8fc0-4757-8dc1-e47aebd35dba"},"outputs":[],"source":["def butter_bandpass(lowcut, highcut, fs, order=3):\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    b, a = butter(order, [low, high], btype='band')\n","    return b, a\n","\n","def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n","    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n","    y = lfilter(b, a, data)\n","    return y\n","\n","def extract_ppg_features(ppg_series, fs):\n","    \"\"\"\n","    Extracts a set of robust, hand-engineered features from a single PPG signal window.\n","    \"\"\"\n","    features = {}\n","\n","    # --- Time-domain features ---\n","    features['ppg_mean'] = np.mean(ppg_series)\n","    features['ppg_std'] = np.std(ppg_series)\n","\n","    # Peak detection and related features\n","    peaks, _ = find_peaks(ppg_series, distance=50, height=0)\n","    if len(peaks) > 1:\n","        # Peak-to-Peak Interval (P-P) in seconds\n","        pp_intervals = np.diff(peaks)\n","        features['mean_pp_interval_s'] = np.mean(pp_intervals) / fs\n","        features['std_pp_interval_s'] = np.std(pp_intervals) / fs\n","        features['ppg_freq'] = fs / np.mean(pp_intervals)\n","    else:\n","        features['mean_pp_interval_s'] = 0\n","        features['std_pp_interval_s'] = 0\n","        features['ppg_freq'] = 0\n","\n","    # Area under the curve (approximated)\n","    features['auc'] = np.trapz(ppg_series)\n","\n","    # Derivatives\n","    derivative = np.diff(ppg_series)\n","    features['first_deriv_max'] = np.max(derivative)\n","    features['first_deriv_min'] = np.min(derivative)\n","\n","    # --- Frequency-domain features ---\n","    hist, _ = np.histogram(ppg_series, bins='auto')\n","    features['entropy'] = entropy(hist)\n","\n","    return features\n","\n","print(\"Utility functions loaded.\")\n","\n"]},{"cell_type":"markdown","id":"c9e280c0-4d63-45cd-bb65-143d3e648d8b","metadata":{"id":"c9e280c0-4d63-45cd-bb65-143d3e648d8b"},"source":["#### 4. Main Data Processing Loop [Example]"]},{"cell_type":"markdown","id":"3a10b5d1-b6c7-47a1-9cbf-a4fcd1edbbe8","metadata":{"id":"3a10b5d1-b6c7-47a1-9cbf-a4fcd1edbbe8"},"source":["This cell downloads data for each patient, applies the filtering and windowing logic, extracts the features, and stores the results."]},{"cell_type":"code","execution_count":null,"id":"a432bc08-b663-4156-bca3-c418b3961e49","metadata":{"id":"a432bc08-b663-4156-bca3-c418b3961e49"},"outputs":[],"source":["# Load clinical information and track list\n","df_cases = pd.read_csv(VITALDB_DATA_URL)\n","df_trks = pd.read_csv(VITALDB_TRACKS_URL)\n","\n","# Filter for cases with demographic and BG data\n","bg_data_cases = df_cases[df_cases['preop_gluc'].notna() & df_cases['age'].notna()].copy()\n","caseids_to_process = list(bg_data_cases['caseid'].unique())\n","\n","print(f\"Found {len(caseids_to_process)} cases with relevant data to process.\")\n","\n","aggregated_data = []\n","processed_case_count = 0\n","\n","# You can limit the number of cases to process for testing\n","for caseid in caseids_to_process:\n","    print(f\"Processing Case ID: {caseid}...\")\n","\n","    # Load raw PPG signal for the case\n","    vals = vitaldb.load_case(caseid, [PPG_SIGNAL_NAME], 1/SAMPLE_RATE_HZ)\n","\n","    if vals is None or vals.size == 0:\n","        print(f\"  No PPG data found for Case ID {caseid}. Skipping.\")\n","        continue\n","\n","    ppg_signal = vals[:, 0]\n","\n","    # Get metadata for the current case\n","    case_meta = bg_data_cases[bg_data_cases['caseid'] == caseid].iloc[0]\n","\n","    # Filter signal within the valid time window around 'opstart' for BG stability\n","    opstart_seconds = case_meta['opstart']\n","    start_idx = max(0, int((opstart_seconds - VALID_WINDOW_MINUTES * 60) * SAMPLE_RATE_HZ))\n","    end_idx = min(len(ppg_signal), int((opstart_seconds + VALID_WINDOW_MINUTES * 60) * SAMPLE_RATE_HZ))\n","\n","    windowed_signal = ppg_signal[start_idx:end_idx]\n","\n","    if len(windowed_signal) < SAMPLES_PER_WINDOW:\n","        print(f\"  Signal for Case ID {caseid} is too short. Skipping.\")\n","        continue\n","\n","    # Apply bandpass filter to the entire windowed signal to remove noise\n","    filtered_signal = butter_bandpass_filter(windowed_signal, lowcut=0.5, highcut=8, fs=SAMPLE_RATE_HZ)\n","\n","    # Iterate through the filtered signal in non-overlapping windows\n","    for i in range(0, len(filtered_signal) - SAMPLES_PER_WINDOW + 1, SAMPLES_PER_WINDOW):\n","        ppg_window = filtered_signal[i:i + SAMPLES_PER_WINDOW]\n","\n","        # Extract hand-engineered features\n","        features = extract_ppg_features(ppg_window, SAMPLE_RATE_HZ)\n","\n","        # Add metadata and target value\n","        features['caseid'] = caseid\n","        features['preop_gluc'] = case_meta['preop_gluc']\n","        features['age'] = case_meta['age']\n","        features['sex'] = 1 if case_meta['sex'] == 'F' else 0\n","        features['preop_dm'] = case_meta['preop_dm']\n","        features['weight'] = case_meta['weight']\n","        features['height'] = case_meta['height']\n","\n","        # Store the complete record\n","        aggregated_data.append(features)\n","\n","    processed_case_count += 1\n","\n","print(f\"\\nProcessed a total of {processed_case_count} cases.\")\n"]},{"cell_type":"markdown","id":"7698c458-2d92-4f4c-b119-97fa782ce4a2","metadata":{"id":"7698c458-2d92-4f4c-b119-97fa782ce4a2"},"source":["#### 5. Create DataFrame and Save to CSV [Example]"]},{"cell_type":"markdown","id":"6096ba92-affe-4fc0-a1d9-404281390fb0","metadata":{"id":"6096ba92-affe-4fc0-a1d9-404281390fb0"},"source":["This final cell converts the list of dictionaries into a pandas DataFrame and saves it to a single CSV file, which is now ready for model training.\n"]},{"cell_type":"code","execution_count":null,"id":"94fb8d05-8a63-422f-9f51-ef171ff513d0","metadata":{"id":"94fb8d05-8a63-422f-9f51-ef171ff513d0"},"outputs":[],"source":["# Convert list of dictionaries to a DataFrame\n","if aggregated_data:\n","    final_df = pd.DataFrame(aggregated_data)\n","    final_df.to_csv(OUTPUT_FILE, index=False)\n","    print(f\"Successfully saved processed data to {OUTPUT_FILE}.\")\n","    print(f\"Final DataFrame shape: {final_df.shape}\")\n","else:\n","    print(\"No data was aggregated. Please check your data source and processing\n","\n"]},{"cell_type":"markdown","id":"73da7198-7c16-4776-aabf-cdc0216d63c5","metadata":{"id":"73da7198-7c16-4776-aabf-cdc0216d63c5"},"source":["## TOGETHER!!. [Note: This worked the last time I ran It]"]},{"cell_type":"code","execution_count":null,"id":"4df36cc9-e12e-4cc3-ad2e-aff09ed43712","metadata":{"id":"4df36cc9-e12e-4cc3-ad2e-aff09ed43712"},"outputs":[],"source":["import pandas as pd\n","import vitaldb\n","import numpy as np\n","import os\n","from scipy.signal import find_peaks, butter, lfilter\n","from scipy.stats import entropy\n","\n","# --- CONFIGURATION ---\n","OUTPUT_DIR = \"./processed_data\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"vitaldb_ppg_extracted_features.csv\")\n","\n","VITALDB_DATA_URL = \"https://api.vitaldb.net/cases\"\n","VITALDB_TRACKS_URL = \"https://api.vitaldb.net/trks\"\n","\n","PPG_SIGNAL_NAME = 'SNUADC/PLETH'\n","SAMPLE_RATE_HZ = 100\n","WINDOW_DURATION_SECONDS = 30\n","SAMPLES_PER_WINDOW = SAMPLE_RATE_HZ * WINDOW_DURATION_SECONDS\n","VALID_WINDOW_MINUTES = 8  # +/- window around 'opstart' for BG stability\n","\n","# --- UTILITY FUNCTIONS ---\n","def butter_bandpass(lowcut, highcut, fs, order=3):\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    b, a = butter(order, [low, high], btype='band')\n","    return b, a\n","\n","def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n","    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n","    y = lfilter(b, a, data)\n","    return y\n","\n","def extract_ppg_features(ppg_series, fs):\n","    \"\"\"\n","    Extracts a set of robust, hand-engineered features from a single PPG signal window.\n","    \"\"\"\n","    features = {}\n","\n","    # Clean the data: remove NaN values\n","    ppg_series_clean = ppg_series[np.isfinite(ppg_series)]\n","\n","    if ppg_series_clean.size == 0:\n","        return {\n","            'ppg_mean': 0, 'ppg_std': 0, 'mean_pp_interval_s': 0,\n","            'std_pp_interval_s': 0, 'ppg_freq': 0, 'auc': 0,\n","            'first_deriv_max': 0, 'first_deriv_min': 0, 'entropy': 0\n","        }\n","\n","    features['ppg_mean'] = np.mean(ppg_series_clean)\n","    features['ppg_std'] = np.std(ppg_series_clean)\n","\n","    peaks, _ = find_peaks(ppg_series_clean, distance=50, height=0)\n","    if len(peaks) > 1:\n","        pp_intervals = np.diff(peaks)\n","        features['mean_pp_interval_s'] = np.mean(pp_intervals) / fs\n","        features['std_pp_interval_s'] = np.std(pp_intervals) / fs\n","        features['ppg_freq'] = fs / np.mean(pp_intervals)\n","    else:\n","        features['mean_pp_interval_s'] = 0\n","        features['std_pp_interval_s'] = 0\n","        features['ppg_freq'] = 0\n","\n","    features['auc'] = np.trapezoid(ppg_series_clean)\n","\n","    derivative = np.diff(ppg_series_clean)\n","    features['first_deriv_max'] = np.max(derivative)\n","    features['first_deriv_min'] = np.min(derivative)\n","\n","    hist, _ = np.histogram(ppg_series_clean, bins='auto')\n","    features['entropy'] = entropy(hist)\n","\n","    return features\n","\n","# --- MAIN DATA PROCESSING LOOP ---\n","print(\"Starting data processing...\")\n","\n","# Load clinical information and track list\n","df_cases = pd.read_csv(VITALDB_DATA_URL)\n","df_trks = pd.read_csv(VITALDB_TRACKS_URL)\n","\n","# Filter for cases with demographic and BG data\n","bg_data_cases = df_cases[df_cases['preop_gluc'].notna() & df_cases['age'].notna()].copy()\n","caseids_to_process = list(bg_data_cases['caseid'].unique())\n","\n","print(f\"Found {len(caseids_to_process)} cases with relevant data to process.\")\n","\n","# Check if the output file already exists to decide whether to write the header\n","output_file_exists = os.path.exists(OUTPUT_FILE)\n","\n","for caseid in caseids_to_process:\n","    print(f\"Processing Case ID: {caseid}...\")\n","\n","    # Load raw PPG signal for the case\n","    vals = vitaldb.load_case(caseid, [PPG_SIGNAL_NAME], 1/SAMPLE_RATE_HZ)\n","\n","    if vals is None or vals.size == 0:\n","        print(f\"  No PPG data found for Case ID {caseid}. Skipping.\")\n","        continue\n","\n","    ppg_signal = vals[:, 0]\n","\n","    # Get metadata for the current case\n","    case_meta = bg_data_cases[bg_data_cases['caseid'] == caseid].iloc[0]\n","\n","    # Filter signal within the valid time window around 'opstart' for BG stability\n","    opstart_seconds = case_meta['opstart']\n","    start_idx = max(0, int((opstart_seconds - VALID_WINDOW_MINUTES * 60) * SAMPLE_RATE_HZ))\n","    end_idx = min(len(ppg_signal), int((opstart_seconds + VALID_WINDOW_MINUTES * 60) * SAMPLE_RATE_HZ))\n","\n","    windowed_signal = ppg_signal[start_idx:end_idx]\n","\n","    if len(windowed_signal) < SAMPLES_PER_WINDOW:\n","        print(f\"  Signal for Case ID {caseid} is too short. Skipping.\")\n","        continue\n","\n","    # Apply bandpass filter to the entire windowed signal to remove noise\n","    filtered_signal = butter_bandpass_filter(windowed_signal, lowcut=0.5, highcut=8, fs=SAMPLE_RATE_HZ)\n","\n","    # Process and save windows for the current case\n","    case_data = []\n","    for i in range(0, len(filtered_signal) - SAMPLES_PER_WINDOW + 1, SAMPLES_PER_WINDOW):\n","        ppg_window = filtered_signal[i:i + SAMPLES_PER_WINDOW]\n","\n","        # Extract hand-engineered features\n","        features = extract_ppg_features(ppg_window, SAMPLE_RATE_HZ)\n","\n","        row = {\n","            'caseid': caseid,\n","            'preop_gluc': case_meta['preop_gluc'],\n","            'age': case_meta['age'],\n","            'sex': 1 if case_meta['sex'] == 'F' else 0,\n","            'preop_dm': case_meta['preop_dm'],\n","            'weight': case_meta['weight'],\n","            'height': case_meta['height'],\n","            **features # Unpack the extracted features\n","        }\n","        case_data.append(row)\n","\n","    # Convert list of dicts to a DataFrame and append to the CSV\n","    case_df = pd.DataFrame(case_data)\n","\n","    # Write the header only once at the beginning if the file is new\n","    header = not output_file_exists\n","    case_df.to_csv(OUTPUT_FILE, mode='a', header=header, index=False)\n","    # Set the flag to False after the first write\n","    output_file_exists = True\n","\n","print(f\"\\nProcessing complete. All data saved to {OUTPUT_FILE}.\")\n","\n","\n"]},{"cell_type":"markdown","id":"1258b4eb-0a9f-4e10-bb72-fe50adc68409","metadata":{"id":"1258b4eb-0a9f-4e10-bb72-fe50adc68409"},"source":["## Example DNN training code [Using Pytorch. Need to change this to Tensforflow]"]},{"cell_type":"code","execution_count":null,"id":"7ade9083-3c3c-4f0f-a54c-27128f5823bb","metadata":{"id":"7ade9083-3c3c-4f0f-a54c-27128f5823bb"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import os\n","\n","# ---- CONFIG ----\n","FILTERED_FILE = '/media/impact/Data/Isaac_BG_SmartWatch/feature_selection/filtered_ppg_features.csv'\n","CASEID_COL = 'caseid'\n","TARGET_COL = 'preop_gluc'\n","BATCH_SIZE = 16\n","EPOCHS = 80\n","LEARNING_RATE = 1e-3\n","DROPOUT = 0.2\n","DNN_LAYERS = [128, 64, 32]\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(\"Loading filtered dataset...\")\n","df = pd.read_csv(FILTERED_FILE)\n","df = df.dropna()\n","print(f\"Loaded shape: {df.shape}\")\n","\n","# ----- Feature/target selection -----\n","features_to_use = [col for col in df.columns if col not in [CASEID_COL, TARGET_COL]]\n","X = df[features_to_use].values.astype(np.float32)\n","y = df[TARGET_COL].values.astype(np.float32)\n","caseids = df[CASEID_COL].values\n","\n","print(f\"Using {len(features_to_use)} features: {features_to_use[:10]} ...\")\n","\n","# ----- Subject-wise train/test split -----\n","unique_caseids = np.unique(caseids)\n","train_ids, test_ids = train_test_split(unique_caseids, test_size=0.2, random_state=42)\n","train_mask = np.isin(caseids, train_ids)\n","test_mask = ~train_mask\n","X_train, X_test = X[train_mask], X[test_mask]\n","y_train, y_test = y[train_mask], y[test_mask]\n","print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n","\n","# ----- Fit scaler only on train! -----\n","x_scaler, y_scaler = MinMaxScaler(), MinMaxScaler()\n","X_train_scaled = x_scaler.fit_transform(X_train)\n","X_test_scaled  = x_scaler.transform(X_test)\n","y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n","y_test_scaled  = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n","y_train_orig, y_test_orig = y_train, y_test\n","\n","# ----- PyTorch DataLoader -----\n","X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n","y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32).unsqueeze(1)\n","X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n","y_test_t = torch.tensor(y_test_scaled, dtype=torch.float32).unsqueeze(1)\n","\n","train_dataset = TensorDataset(X_train_t, y_train_t)\n","test_dataset  = TensorDataset(X_test_t, y_test_t)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","\n","# ----- DNN Model -----\n","class DNNRegressor(nn.Module):\n","    def __init__(self, in_features, layers, dropout):\n","        super().__init__()\n","        self.seq = nn.Sequential(\n","            nn.Linear(in_features, layers[0]),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(layers[0]),\n","            nn.Dropout(dropout),\n","            nn.Linear(layers[0], layers[1]),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(layers[1]),\n","            nn.Dropout(dropout),\n","            nn.Linear(layers[1], layers[2]),\n","            nn.ReLU(),\n","            nn.Linear(layers[2], 1)\n","        )\n","    def forward(self, x):\n","        return self.seq(x)\n","\n","model = DNNRegressor(X_train_t.shape[1], DNN_LAYERS, DROPOUT).to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","criterion = nn.MSELoss()\n","\n","# ----- Training Loop -----\n","train_losses, val_losses = [], []\n","print(\"Training DNN model...\")\n","for epoch in range(EPOCHS):\n","    model.train()\n","    epoch_train_loss = 0\n","    for xb, yb in train_loader:\n","        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n","        optimizer.zero_grad()\n","        pred = model(xb)\n","        loss = criterion(pred, yb)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss.item() * xb.size(0)\n","    epoch_train_loss /= len(train_loader.dataset)\n","    train_losses.append(epoch_train_loss)\n","\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n","            pred = model(xb)\n","            val_loss += criterion(pred, yb).item() * xb.size(0)\n","    val_loss /= len(test_loader.dataset)\n","    val_losses.append(val_loss)\n","    if (epoch+1) % 10 == 0 or epoch < 5:\n","        print(f\"Epoch {epoch+1}: train {epoch_train_loss:.4f} val {val_loss:.4f}\")\n","\n","# ----- Evaluation -----\n","model.eval()\n","y_pred_scaled = []\n","with torch.no_grad():\n","    for xb, _ in test_loader:\n","        xb = xb.to(DEVICE)\n","        preds = model(xb).cpu().numpy().flatten()\n","        y_pred_scaled.append(preds)\n","y_pred_scaled = np.concatenate(y_pred_scaled)\n","y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n","mae = mean_absolute_error(y_test_orig, y_pred)\n","mape = mean_absolute_percentage_error(y_test_orig, y_pred) * 100\n","print(f\"\\nTest MAE: {mae:.2f} mg/dL\")\n","print(f\"Test MAPE: {mape:.2f}%\")\n","\n","# ----- Plots -----\n","plt.figure(figsize=(8, 4))\n","plt.plot(train_losses, label='Train Loss (MSE)')\n","plt.plot(val_losses, label='Val Loss (MSE)')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend(); plt.tight_layout(); plt.show()\n","\n","plt.figure(figsize=(7, 7))\n","plt.scatter(y_test_orig, y_pred, alpha=0.5, label='Test Predictions')\n","slope, intercept = np.polyfit(y_test_orig, y_pred, 1)\n","plt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], 'k--', label='Ideal (y=x)')\n","plt.plot([y_test_orig.min(), y_test_orig.max()],\n","         [slope * y_test_orig.min() + intercept, slope * y_test_orig.max() + intercept],\n","         color='red', lw=2, label=f'Trendline (slope={slope:.2f})')\n","plt.xlabel(\"Actual BG (mg/dL)\")\n","plt.ylabel(\"Predicted BG (mg/dL)\")\n","plt.title(\"Actual vs. Predicted BG (PyTorch DNN)\")\n","plt.legend(); plt.tight_layout(); plt.show()\n","print(f\"Scatter plot trendline slope: {slope:.2f}\")\n","\n","# Save PyTorch model\n","save_path = '/media/impact/Data/Isaac_BG_SmartWatch/bg_best_dnn_model2.pt'\n","torch.save(model.state_dict(), save_path)\n","print(f\"Model saved to {save_path}\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}