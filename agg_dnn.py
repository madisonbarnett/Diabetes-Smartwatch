# --------------------------------------------------------------
# agg_seq_dense.py  (improved version - generated by Grok)
# --------------------------------------------------------------
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers

# -------------------- CONFIG --------------------
FILTERED_FILE = './processed_data/vitaldb_ppg_ecg_extracted_features_30s.csv'
CASEID_COL    = 'caseid'
TARGET_COL    = 'preop_gluc'

# sequential features (the ones that vary over time)
SEQ_FEATURES = [
    'mean_bp', 'sys_bp', 'dys_bp',
    'ppg_mean', 'ppg_std', 'mean_pp_interval_s', 'std_pp_interval_s',
    'ppg_freq', 'auc', 'first_deriv_max', 'first_deriv_min', 'entropy',
    'ecg_mean', 'ecg_std', 'ecg_mean_pp_interval_s', 'ecg_std_pp_interval_s',
    'ecg_freq', 'ecg_auc', 'ecg_first_deriv_max', 'ecg_first_deriv_min', 'ecg_entropy'
]

# static covariates (constant per subject)
STATIC_FEATURES = ['age', 'sex', 'preop_dm', 'weight', 'height']

# hyper-parameters
EPOCHS          = 100
BATCH_SIZE      = 64
LEARNING_RATE   = 1e-3
DROPOUT         = 0.2
DNN_LAYERS      = [128, 64, 32]
PATIENCE_ES     = 20
PATIENCE_LR     = 10
MIN_DELTA       = 1e-4

# --------------------------------------------------------------
print("Loading data …")
df = pd.read_csv(FILTERED_FILE).dropna()
print(f"Raw shape: {df.shape}")

# --------------------------------------------------------------
# 1. Aggregate sequential features per caseid
# --------------------------------------------------------------
def aggregate_per_subject(df):
    agg_dict = {f: 'mean' for f in SEQ_FEATURES}
    for f in STATIC_FEATURES + [TARGET_COL]:
        agg_dict[f] = 'first'                     # static → take any value
    agg_dict[CASEID_COL] = 'first'

    agg = df.groupby(CASEID_COL).agg(agg_dict).reset_index(drop=True)
    return agg

agg_df = aggregate_per_subject(df)
print(f"Aggregated shape (one row per subject): {agg_df.shape}")

# --------------------------------------------------------------
# 2. Build X / y
# --------------------------------------------------------------
feature_cols = SEQ_FEATURES + STATIC_FEATURES
X = agg_df[feature_cols].values.astype(np.float32)
y = agg_df[TARGET_COL].values.astype(np.float32)
caseids = agg_df[CASEID_COL].values

# --------------------------------------------------------------
# 3. Subject-wise train / test split
# --------------------------------------------------------------
unique_ids = np.unique(caseids)
train_ids, test_ids = train_test_split(
    unique_ids, test_size=0.2, random_state=42, stratify=None
)

train_mask = np.isin(caseids, train_ids)
test_mask  = ~train_mask

X_train, X_test = X[train_mask], X[test_mask]
y_train, y_test = y[train_mask], y[test_mask]

print(f"Train subjects: {len(train_ids)}  →  {X_train.shape[0]} rows")
print(f"Test  subjects: {len(test_ids)}  →  {X_test.shape[0]} rows")

# --------------------------------------------------------------
# 4. Scaling (fit on train only!)
# --------------------------------------------------------------
x_scaler = MinMaxScaler()          # or StandardScaler() – both work
y_scaler = MinMaxScaler()

X_train_s = x_scaler.fit_transform(X_train)
X_test_s  = x_scaler.transform(X_test)

y_train_s = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test_s  = y_scaler.transform(y_test.reshape(-1, 1)).flatten()

# keep originals for reporting
y_train_orig, y_test_orig = y_train.copy(), y_test.copy()

# --------------------------------------------------------------
# 5. TF Datasets
# --------------------------------------------------------------
train_ds = tf.data.Dataset.from_tensor_slices((X_train_s, y_train_s))
test_ds  = tf.data.Dataset.from_tensor_slices((X_test_s,  y_test_s))

train_ds = train_ds.shuffle(buffer_size=len(X_train_s)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_ds  = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# --------------------------------------------------------------
# 6. DNN model
# --------------------------------------------------------------
class DNNRegressor(tf.keras.Model):
    def __init__(self, in_dim, layer_sizes, dropout):
        super().__init__()
        seq = [
            layers.Dense(layer_sizes[0], input_shape=(in_dim,)),  # ← layer_sizes[0]
            layers.ReLU(),
            layers.BatchNormalization(),
            layers.Dropout(dropout)
        ]

        for units in layer_sizes[1:]:
            seq += [
                layers.Dense(units),
                layers.ReLU(),
                layers.BatchNormalization(),
                layers.Dropout(dropout)
            ]

        seq += [layers.Dense(1)]
        self.net = models.Sequential(seq)

    def call(self, x, training=False):
        return self.net(x, training=training)

# Now instantiate
model = DNNRegressor(X_train_s.shape[1], DNN_LAYERS, DROPOUT)
model.compile(optimizer=optimizers.Adam(LEARNING_RATE),
              loss='mse',
              metrics=['mae'])

# --------------------------------------------------------------
# 7. Callbacks
# --------------------------------------------------------------
cb_es  = callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE_ES,
                                 restore_best_weights=True, min_delta=MIN_DELTA)
cb_lr  = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                                     patience=PATIENCE_LR, min_lr=1e-6)
# cb_tb  = callbacks.TensorBoard(log_dir='./tb_logs/agg_dense', histogram_freq=1)

# --------------------------------------------------------------
# 8. Train
# --------------------------------------------------------------
history = model.fit(train_ds,
                    epochs=EPOCHS,
                    validation_data=test_ds,
                    callbacks=[cb_es, cb_lr],
                    verbose=1)

# --------------------------------------------------------------
# 9. Evaluation (inverse-transform → original scale)
# --------------------------------------------------------------
y_pred_s = model.predict(test_ds, verbose=0).flatten()
y_pred   = y_scaler.inverse_transform(y_pred_s.reshape(-1, 1)).flatten()

mae  = mean_absolute_error(y_test_orig, y_pred)
mape = mean_absolute_percentage_error(y_test_orig, y_pred) * 100
r2   = r2_score(y_test_orig, y_pred)

print("\n=== FINAL METRICS (original mg/dL) ===")
print(f"MAE  : {mae:6.2f} mg/dL")
print(f"MAPE : {mape:6.2f} %")
print(f"R²   : {r2:6.3f}")

# --------------------------------------------------------------
# 10. Plots (identical style to dnn.py)
# --------------------------------------------------------------
# 10.1 Loss curves
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'],     label='Train MSE')
plt.plot(history.history['val_loss'], label='Val   MSE')
plt.title('Training / Validation Loss')
plt.xlabel('Epoch'); plt.ylabel('MSE (scaled)')
plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
plt.show()

# 10.2 Scatter + trendline
plt.figure(figsize=(7,7))
plt.scatter(y_test_orig, y_pred, alpha=0.6, s=30, edgecolor='k', label='Predictions')

# y=x line
lim = [min(y_test_orig.min(), y_pred.min()),
       max(y_test_orig.max(), y_pred.max())]
plt.plot(lim, lim, 'k--', lw=1, label='Ideal (y=x)')

# regression line
slope, intercept = np.polyfit(y_test_orig, y_pred, 1)
x_reg = np.array(lim)
plt.plot(x_reg, slope*x_reg + intercept, 'r-', lw=2,
         label=f'Trend (slope={slope:.2f})')

plt.xlabel('Actual BG (mg/dL)')
plt.ylabel('Predicted BG (mg/dL)')
plt.title('Actual vs Predicted Blood Glucose')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 10.3 Bland-Altman
# diff = y_pred - y_test_orig
# mean_diff = diff.mean()
# std_diff  = diff.std()

# plt.figure(figsize=(8,5))
# plt.scatter(y_test_orig, diff, alpha=0.6, s=30, edgecolor='k')
# plt.axhline(mean_diff, color='gray', linestyle='--', label=f'Mean diff = {mean_diff:+.2f}')
# plt.axhline(mean_diff + 1.96*std_diff, color='red',  linestyle='--',
#             label=f'+1.96σ = {mean_diff+1.96*std_diff:+.2f}')
# plt.axhline(mean_diff - 1.96*std_diff, color='red',  linestyle='--',
#             label=f'-1.96σ = {mean_diff-1.96*std_diff:+.2f}')
# plt.xlabel('Mean of Actual & Predicted (mg/dL)')
# plt.ylabel('Prediction – Actual (mg/dL)')
# plt.title('Bland-Altman Plot')
# plt.legend()
# plt.grid(alpha=0.3)
# plt.tight_layout()
# plt.show()

# --------------------------------------------------------------
# 11. Save everything for later inference
# --------------------------------------------------------------
save_dir = './model_weights'
os.makedirs(save_dir, exist_ok=True)

# model weights
model.save_weights(save_dir + '/agg_dense.weights.h5')

print(f"\nModel weights saved to {save_dir}")