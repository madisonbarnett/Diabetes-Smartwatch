# --------------------------------------------------------------
# agg_seq_dense.py  (improved version - generated by Grok)
# --------------------------------------------------------------
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers

# -------------------- CONFIG --------------------
FILTERED_FILE = './processed_data/vitaldb_ppg_ecg_extracted_features_10s.csv'
CASEID_COL    = 'caseid'
TARGET_COL    = 'preop_gluc'

# sequential features (vary over time)
SEQ_FEATURES = [
    'mean_bp', 'sys_bp', 'dys_bp',
    'ppg_mean', 'ppg_std', 'mean_pp_interval_s', 'std_pp_interval_s',
    'ppg_freq', 'auc', 'first_deriv_max', 'first_deriv_min', 'entropy',
    'ecg_mean', 'ecg_std', 'ecg_mean_pp_interval_s', 'ecg_std_pp_interval_s',
    'ecg_freq', 'ecg_auc', 'ecg_first_deriv_max', 'ecg_first_deriv_min', 'ecg_entropy'
]

# static features
STATIC_FEATURES = ['age', 'sex', 'preop_dm', 'weight', 'height']

# hyper-parameters
EPOCHS          = 1
BATCH_SIZE      = 16
LEARNING_RATE   = 1e-3
DROPOUT         = 0.2
# DNN_LAYERS      = [128, 64, 32]
DNN_LAYERS      = [64, 32]
PATIENCE_ES     = 20
PATIENCE_LR     = 10
MIN_DELTA       = 1e-4

# --------------------------------------------------------------
print("Loading data …")
df = pd.read_csv(FILTERED_FILE).dropna()
print(f"Raw shape: {df.shape}")

# --------------------------------------------------------------
# 1. Aggregate sequential features per caseid
# --------------------------------------------------------------
def aggregate_per_subject(df):
    agg_dict = {f: 'mean' for f in SEQ_FEATURES}
    for f in STATIC_FEATURES + [TARGET_COL]:
        agg_dict[f] = 'first'                     
    agg_dict[CASEID_COL] = 'first'

    agg = df.groupby(CASEID_COL).agg(agg_dict).reset_index(drop=True)
    return agg

agg_df = aggregate_per_subject(df)
print(f"Aggregated shape (one row per subject): {agg_df.shape}")

# --------------------------------------------------------------
# 2. Build X / y
# --------------------------------------------------------------
feature_cols = SEQ_FEATURES + STATIC_FEATURES
X = agg_df[feature_cols].values.astype(np.float32)
y = agg_df[TARGET_COL].values.astype(np.float32)
caseids = agg_df[CASEID_COL].values

# --------------------------------------------------------------
# 3. Subject-wise train / test split
# --------------------------------------------------------------
unique_ids = np.unique(caseids)
train_ids, test_ids = train_test_split(
    unique_ids, test_size=0.2, random_state=42, stratify=None
)

train_mask = np.isin(caseids, train_ids)
test_mask  = ~train_mask

X_train, X_test = X[train_mask], X[test_mask]
y_train, y_test = y[train_mask], y[test_mask]

print(f"Train subjects: {len(train_ids)}  →  {X_train.shape[0]} rows")
print(f"Test  subjects: {len(test_ids)}  →  {X_test.shape[0]} rows")

# --------------------------------------------------------------
# 4. Scaling (fit on train only!)
# --------------------------------------------------------------
x_scaler = MinMaxScaler()          # or StandardScaler() – both work
y_scaler = MinMaxScaler()

X_train_s = x_scaler.fit_transform(X_train)
X_test_s  = x_scaler.transform(X_test)

y_train_s = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test_s  = y_scaler.transform(y_test.reshape(-1, 1)).flatten()

# keep originals for reporting
y_train_orig, y_test_orig = y_train.copy(), y_test.copy()

# --------------------------------------------------------------
# 5. TF Datasets
# --------------------------------------------------------------
train_ds = tf.data.Dataset.from_tensor_slices((X_train_s, y_train_s))
test_ds  = tf.data.Dataset.from_tensor_slices((X_test_s,  y_test_s))

train_ds = train_ds.shuffle(buffer_size=len(X_train_s)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_ds  = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# --------------------------------------------------------------
# 6. DNN model
# --------------------------------------------------------------
class DNNRegressor(tf.keras.Model):
    def __init__(self, in_dim, layer_sizes, dropout):
        super().__init__()
        seq = [
            layers.Dense(layer_sizes[0], input_shape=(in_dim,)),  # ← layer_sizes[0]
            layers.ReLU(),
            layers.BatchNormalization(),
            layers.Dropout(dropout)
        ]

        for units in layer_sizes[1:]:
            seq += [
                layers.Dense(units),
                layers.ReLU(),
                layers.BatchNormalization(),
                layers.Dropout(dropout)
            ]

        seq += [layers.Dense(1)]
        self.net = models.Sequential(seq)

    def call(self, x, training=False):
        return self.net(x, training=training)

# Now instantiate
model = DNNRegressor(X_train_s.shape[1], DNN_LAYERS, DROPOUT)
model.compile(optimizer=optimizers.Adam(LEARNING_RATE),
              loss='mse',
              metrics=['mae'])

# --------------------------------------------------------------
# 7. Callbacks
# --------------------------------------------------------------
cb_es  = callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE_ES,
                                 restore_best_weights=True, min_delta=MIN_DELTA)
cb_lr  = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                                     patience=PATIENCE_LR, min_lr=1e-6)
# cb_tb  = callbacks.TensorBoard(log_dir='./tb_logs/agg_dense', histogram_freq=1)

# --------------------------------------------------------------
# 8. Train
# --------------------------------------------------------------
history = model.fit(train_ds,
                    epochs=EPOCHS,
                    validation_data=test_ds,
                    callbacks=[cb_es, cb_lr],
                    verbose=1)

# --------------------------------------------------------------
# 9. Evaluation (inverse-transform → original scale)
# --------------------------------------------------------------
y_pred_s = model.predict(test_ds, verbose=0).flatten()
y_pred   = y_scaler.inverse_transform(y_pred_s.reshape(-1, 1)).flatten()

mae  = mean_absolute_error(y_test_orig, y_pred)
mape = mean_absolute_percentage_error(y_test_orig, y_pred) * 100
r2   = r2_score(y_test_orig, y_pred)

print("\n=== FINAL METRICS (original mg/dL) ===")
print(f"MAE  : {mae:6.2f} mg/dL")
print(f"MAPE : {mape:6.2f} %")
print(f"R²   : {r2:6.3f}")

# Clarke Error Grid Analysis
def get_clarke_zone(ref, pred):
    # Force numeric (helps when using pandas)
    r, p = float(ref), float(pred)

    if (r <= 70 and p <= 70) or (0.8*r <= p <= 1.2*r):
        return 'A'

    if (130 < r <= 180 and 1.4*(r-130) >= p) or (70 < r <= 280 and p >= (r+110)):
        return 'C'

    if (r <= 70 and 70 < p <= 180) or (r >= 240 and 70 <= p <= 180):
        return 'D'

    if (r <= 70 and p > 180) or (r > 180 and p <= 70):
        return 'E'

    return 'B'      # everything else

zones_count = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}
total_points = len(y_test_orig)
points = []  # Store (ref, pred, zone) for plotting

for ref, pred in zip(y_test_orig, y_pred):
    zone = get_clarke_zone(ref, pred)
    zones_count[zone] += 1
    points.append((ref, pred, zone))

print("Clarke Error Grid Analysis:")
for zone, count in zones_count.items():
    percentage = (count / total_points) * 100
    print(f"Zone {zone}: {percentage:.2f}% ({count}/{total_points} points)")

# --- Plotting Clarke Error Grid ---
plt.figure(figsize=(10, 10))

# Define zone colors
colors = {'A': 'green', 'B': 'yellow', 'C': 'orange', 'D': 'red', 'E': 'purple'}
labels = {
    'A': 'A: Clinically Accurate',
    'B': 'B: Benign Errors',
    'C': 'C: Overcorrection',
    'D': 'D: Dangerous Failure to Detect',
    'E': 'E: Erroneous Treatment'
}

# Scatter points by zone
for zone in 'ABCDE':
    zone_points = [(r, p) for r, p, z in points if z == zone]
    if zone_points:
        refs, preds = zip(*zone_points)
        plt.scatter(refs, preds, c=colors[zone], label=f"{labels[zone]} ({zones_count[zone]})", alpha=0.7, edgecolors='k', s=60)

# Draw grid boundaries
max_val = 400
x = np.linspace(0, max_val, 500)

# Perfect line (y = x)
plt.plot([0, max_val], [0, max_val], 'k--', linewidth=1, label='Perfect Agreement')

# Zone A boundaries: ±20% or within 70
plt.fill_between(x, 0.8*x, 1.2*x, where=(x <= 70) | (x >= 70), color='green', alpha=0.1, label='_nolegend_')
plt.fill_between(x, 0, 70, where=x <= 70, color='green', alpha=0.1)

# Zone B: outside A but safe
# Complex boundaries, draw other zones instead

# Zone C: 
plt.fill([70, 70, 290], [180, 400, 400], color='orange', alpha=0.4)
plt.fill([130, 180, 180], [0, 0, 70], color='orange', alpha=0.4)

# Zone D: 
plt.axhspan(70, 180, xmin=0, xmax=70/400, color='red', alpha=0.1)
plt.axhspan(70, 180, xmin=240/400, xmax=1, color='red', alpha=0.1)

# Zone E: 
plt.axhspan(180, max_val, xmin=0, xmax=70/400, color='purple', alpha=0.1)
plt.axhspan(0, 70, xmin=180/400, xmax=1, color='purple', alpha=0.1)

# Axis limits and labels
plt.xlim(0, max_val)
plt.ylim(0, max_val)
plt.xlabel('Reference Glucose (mg/dL)', fontsize=12)
plt.ylabel('Predicted Glucose (mg/dL)', fontsize=12)
plt.title('Clarke Error Grid Analysis', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.5)

# Equal aspect ratio
plt.gca().set_aspect('equal', adjustable='box')

# Legend
plt.legend(loc='upper left')

# Show plot
plt.tight_layout()
plt.show()
    

# --------------------------------------------------------------
# 10. Plots (identical style to dnn.py)
# --------------------------------------------------------------
# 10.1 Loss curves
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'],     label='Train MSE')
plt.plot(history.history['val_loss'], label='Val   MSE')
plt.title('Training / Validation Loss')
plt.xlabel('Epoch'); plt.ylabel('MSE (scaled)')
plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
plt.show()

# 10.2 Scatter + trendline
plt.figure(figsize=(7,7))
plt.scatter(y_test_orig, y_pred, alpha=0.6, s=30, edgecolor='k', label='Predictions')

# y=x line
lim = [min(y_test_orig.min(), y_pred.min()),
       max(y_test_orig.max(), y_pred.max())]
plt.plot(lim, lim, 'k--', lw=1, label='Ideal (y=x)')

# regression line
slope, intercept = np.polyfit(y_test_orig, y_pred, 1)
x_reg = np.array(lim)
plt.plot(x_reg, slope*x_reg + intercept, 'r-', lw=2,
         label=f'Trend (slope={slope:.2f})')

plt.xlabel('Actual BG (mg/dL)')
plt.ylabel('Predicted BG (mg/dL)')
plt.title('Actual vs Predicted Blood Glucose')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 10.3 Bland-Altman
# diff = y_pred - y_test_orig
# mean_diff = diff.mean()
# std_diff  = diff.std()

# plt.figure(figsize=(8,5))
# plt.scatter(y_test_orig, diff, alpha=0.6, s=30, edgecolor='k')
# plt.axhline(mean_diff, color='gray', linestyle='--', label=f'Mean diff = {mean_diff:+.2f}')
# plt.axhline(mean_diff + 1.96*std_diff, color='red',  linestyle='--',
#             label=f'+1.96σ = {mean_diff+1.96*std_diff:+.2f}')
# plt.axhline(mean_diff - 1.96*std_diff, color='red',  linestyle='--',
#             label=f'-1.96σ = {mean_diff-1.96*std_diff:+.2f}')
# plt.xlabel('Mean of Actual & Predicted (mg/dL)')
# plt.ylabel('Prediction – Actual (mg/dL)')
# plt.title('Bland-Altman Plot')
# plt.legend()
# plt.grid(alpha=0.3)
# plt.tight_layout()
# plt.show()

# --------------------------------------------------------------
# 11. Save everything for later inference
# --------------------------------------------------------------
save_dir = './model_weights'
os.makedirs(save_dir, exist_ok=True)

# model weights
model.save_weights(save_dir + '/agg_dense_10s.weights.h5')

print(f"\nModel weights saved to {save_dir}")